<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>How it works &mdash; Macro Random Forest 1.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Docs" href="modules.html" />
    <link rel="prev" title="Macroeconomic Random Forest" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Macro Random Forest
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">How it works</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#random-forest">Random Forest</a></li>
<li class="toctree-l2"><a class="reference internal" href="#random-walk-regularisation">Random Walk Regularisation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Docs</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Macro Random Forest</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>How it works</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="how-it-works">
<h1>How it works<a class="headerlink" href="#how-it-works" title="Permalink to this headline"></a></h1>
<p>This is a simple explanation of MRF, how it works and why it’s useful. The algorithm is described in more detail in <a class="reference external" href="https://arxiv.org/abs/2006.12724">https://arxiv.org/abs/2006.12724</a>.</p>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline"></a></h2>
<p>Within the modern ML canon, random forest is an extremely popular algorithm because it allows for complex nonlinearities, handled high-dimensional data, bypasses overfitting, and requires little to no tuning. However, while random forest gladly delivers gains in prediction accuracy (and ergo a conditional mean closer to the truth), it is much more reluctant to disclose its inherent model.</p>
<p>MRF shifts the focus of the forest away from predicting <span class="math notranslate nohighlight">\(y_t\)</span> into modelling <span class="math notranslate nohighlight">\(\beta_t\)</span>, which are the economically meaningful coefficients in a time-varying linear macro equation. More formally:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\begin{aligned}
y_t = X_t \beta_t  + \varepsilon_t
\end{aligned}
\end{equation*}\]</div>
<div class="math notranslate nohighlight">
\[\begin{equation}
\beta_t = \mathcal{F}(S_t)
\end{equation}\]</div>
<p>Where <span class="math notranslate nohighlight">\(S_t\)</span> are the state variables governing time variation and <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is a forest. <span class="math notranslate nohighlight">\(X_t\)</span> is typically a subset of <span class="math notranslate nohighlight">\(S_t\)</span> which we want to be time-varying. This setup provides strong generality. For instance, <span class="math notranslate nohighlight">\(X_t\)</span> could use lags of <span class="math notranslate nohighlight">\(y_t\)</span> - what is called an autoregressive random forest (ARRF). Typically <span class="math notranslate nohighlight">\(X_t \subset S_t\)</span> is rather small (and focused) compared to <span class="math notranslate nohighlight">\(S_t\)</span>.</p>
<p>The beauty of the setup resides in combining the linear macro equation with the random forest ML algorithm. This allows our linear coefficient, which we can interpret and make inference about, to nest the important time-series nonlinearities captured by the forest.</p>
</section>
<section id="random-forest">
<h2>Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline"></a></h2>
<p>For those unfamiliar with random forests, the general fitting procedure involves firstly bootstrapping the data to create a random sub-sample of observations. In time series, this will be a set of time indices <span class="math notranslate nohighlight">\(l\)</span> that becomes the parent node for our tree-splitting procedure.</p>
<p>After randomising over rows, we then take a random subset of the predictors, call it <span class="math notranslate nohighlight">\(\mathcal{J}^-\)</span>. MRF then performs a search for the optimal predictor and optimal splitting point. For each tree, we  implement least squares optimisation with a ridge penalty over <span class="math notranslate nohighlight">\(j \in \mathcal{J}^{-}\)</span> and <span class="math notranslate nohighlight">\(c \in \mathbb{R}\)</span>, where c is the splitting point. Mathematically, this becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation*}
\begin{aligned}
\begin{aligned}\label{OLS}
(j^*, c^*) = \min _{j \in \mathcal{J}^{-}, \; c \in \mathbb{R}} &amp;\left[\min _{\beta_{1}} \sum_{\left\{t \in l \mid S_{j, t} \leq c\right\}}\left(y_{t}-X_{t} \beta_{1}\right)^{2}+\lambda\left\|\beta_{1}\right\|_{2}\right.\\
 &amp;\left.+\min _{\beta_{2}} \sum_{\left\{t \in l \mid S_{j, t}&gt;c\right\}}\left(y_{t}-X_{t} \beta_{2}\right)^{2}+\lambda\left\|\beta_{2}\right\|_{2}\right]
\end{aligned}
\end{aligned} \label{a} \tag{1}
\end{equation*}\end{split}\]</div>
<p>Practically, optimisation over <span class="math notranslate nohighlight">\(c\)</span> happens by sampling empirical quantiles of the predictor to be split. These become the possible options for the splits and we evaluate least squares repeatedly to find the optimal splitting point for a given predictor <span class="math notranslate nohighlight">\(j\)</span>. In an outer loop, we take the minimum to find <span class="math notranslate nohighlight">\(j^* \in \mathcal{J}^{-}\)</span> and <span class="math notranslate nohighlight">\(c^* \in \mathbb{R}\)</span>.</p>
<p>This process is, in principle, a greedy search algorithm. A greedy algorithm makes “locally” optimal decisions, rather than finding the globally optimal solution.</p>
<img alt="_images/Greedy_v_true.svg" src="_images/Greedy_v_true.svg" /><p>However, various properties of random forests reduce the extent to which this is a problem in practice. First, each tree is grown on a bootstrapped sample, meaning that we are selecting many observation triplets <span class="math notranslate nohighlight">\([y_t, X_t, S_t]\)</span> for each tree that is fit. This means the trees are diversified by being fit on many different random subsamples. By travelling down a wide array of optimization routes, the forest safeguards against landing at a single greedy solution.</p>
<p>This problem is further alleviated in our context by growing trees semi-stochastically. In Equation <span class="math notranslate nohighlight">\(\ref{a}\)</span>, this is made operational by using <span class="math notranslate nohighlight">\(\mathcal{J}^{-} \in \mathcal{J}\)</span> rather than <span class="math notranslate nohighlight">\(\mathcal{J}\)</span>. This means that at each step of the recursion, a different subsample of regressors is drawn to constitute candidates for the split. This prevents the greedy algorithm from always embarking on the same optimization route. As a result, trees are further diversified and computing time reduced.</p>
</section>
<section id="random-walk-regularisation">
<h2>Random Walk Regularisation<a class="headerlink" href="#random-walk-regularisation" title="Permalink to this headline"></a></h2>
<p>Equation <span class="math notranslate nohighlight">\(\ref{a}\)</span> uses Ridge shrinkage which implies that each time-varying coefficient (<span class="math notranslate nohighlight">\(\beta_t\)</span>) is implicitly shrunk to 0 at every point in time. This can be an issue if a process is highly persistent, since shrinking the first lag heavily to 0 can incur serious bias. <span class="math notranslate nohighlight">\(\beta_i = 0\)</span> is a natural stochastic constraint in a cross-sectional setting, but its time series translation <span class="math notranslate nohighlight">\(\beta_t = 0\)</span> can easily be suboptimal. The traditional regularisation employed in macro is rather the random walk:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\begin{aligned}
\begin{aligned}
\beta_t = \beta_{t-1} + u_t
\end{aligned}
\end{aligned}
\end{equation*}\]</div>
<p>Thus it is desirable to transform Equation <span class="math notranslate nohighlight">\(\ref{a}\)</span> so that that coefficients evolve smoothly, which entails shrinking <span class="math notranslate nohighlight">\(\beta_t\)</span> to be in the neighborhood of <span class="math notranslate nohighlight">\(\beta_{t-1}\)</span> and <span class="math notranslate nohighlight">\(\beta_{t+1}\)</span> rather than 0. This is in line with the view that economic states last for at least a few consecutive periods.</p>
<p>This regularisation is implemented by taking the rolling-window view of time-varying parameters. That is, the tree, instead of solving a plethora of small ridge problems, will rather solve many weighted least squares (WLS) problems, which includes close-by observations. The latter are in the neighborhood (in time) of observations in the current leaf. They are included in the estimation, but are allocated a smaller weight. For simplicity and to keep the computational demand low, the kernel used by WLS is a simple symmetric 5-step Olympic podium.</p>
<p>Informally, the kernel puts a weight of 1 on observation  <span class="math notranslate nohighlight">\(t\)</span>, a weight of <span class="math notranslate nohighlight">\(\zeta &lt; 1\)</span> for observations <span class="math notranslate nohighlight">\(t-1\)</span> and <span class="math notranslate nohighlight">\(t+1\)</span> and a weight of <span class="math notranslate nohighlight">\(\zeta^2\)</span> for observations <span class="math notranslate nohighlight">\(t-2\)</span> and <span class="math notranslate nohighlight">\(t+2\)</span>. Since some specific <span class="math notranslate nohighlight">\(t\)</span>’s will come up many times (for instance if observations <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(t+1\)</span> are in the same leaf), MRF takes the maximal weight allocated to <span class="math notranslate nohighlight">\(t\)</span> as the final weight <span class="math notranslate nohighlight">\(w(t; \zeta)\)</span>.</p>
<p>Formally, define <span class="math notranslate nohighlight">\(l_{-1}\)</span> as the lagged version of th leaf <span class="math notranslate nohighlight">\(l\)</span>. In other words <span class="math notranslate nohighlight">\(l_{-1}\)</span> is a set containing each observation from <span class="math notranslate nohighlight">\(l\)</span>, with all of them lagged one step. <span class="math notranslate nohighlight">\(l_{+1}\)</span> is the “forwarded” version. <span class="math notranslate nohighlight">\(l_{-2}\)</span> and <span class="math notranslate nohighlight">\(l_{+2}\)</span> are two-steps equivalents. For a given candidate subsample <span class="math notranslate nohighlight">\(l\)</span>, the podium is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}w(t ; \zeta)=\left\{\begin{array}{ll}
1, &amp; \text { if } t \in l \\
\zeta, &amp; \text { if } t \in\left(l_{+1} \cup l_{-1}\right) / l \\
\zeta^{2}, &amp; \text { if } t \in\left(l_{+2} \cup l_{-2}\right) /\left(l \cup\left(l_{+1} \cup l_{-1}\right)\right) \\
0, &amp; \text { otherwise }
\end{array}\right.\end{split}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\zeta &lt; 1\)</span> is the tuning parameter guiding the level of time-smoothing. Then, it is only a matter of how to include those additional (but down weighted) observations in the tree search procedure. The usual candidate splitting sets:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\begin{aligned}
\begin{aligned}
l_{1}(j, c) \equiv\left\{t \in l \mid S_{j, t} \leq c\right\} \quad \text { and } \quad l_{2}(j, c) \equiv\left\{t \in l \mid S_{j, t}&gt;c\right\}
\end{aligned}
\end{aligned}
\end{equation*}\]</div>
<p>are expanded to include all observations of relevance to the podium:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\begin{aligned}
\begin{aligned}
\text { for } i=1,2: \quad l_{i}^{RW}(j, c) \equiv l_{i}(j, c) \cup l_{i}(j, c)_{-1} \cup l_{i}(j, c)_{+1} \cup l_{i}(j, c)_{-2} \cup l_{i}(j, c)_{+2}
\end{aligned}
\end{aligned}
\end{equation*}\]</div>
<p>The splitting rule then becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation*}
\begin{aligned}
\begin{aligned}
(j^*, c^*) = \min _{j \in \mathcal{J}^{-}, c \in \mathbb{R}} &amp; {\left[\min _{\beta_{1}} \sum_{t \in l_{1}^{R W}(j, c)} w(t ; \zeta)\left(y_{t}-X_{t} \beta_{1}\right)^{2}+\lambda\left\|\beta_{1}\right\|_{2}\right.} \\
&amp;\left.+\min _{\beta_{2}} \sum_{t \in l_{2}^{ RW}(j, c)} w(t ; \zeta)\left(y_{t}-X_{t} \beta_{2}\right)^{2}+\lambda\left\|\beta_{2}\right\|_{2}\right]
\end{aligned}
\end{aligned} \label{b} \tag{2}
\end{equation*}\end{split}\]</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Macroeconomic Random Forest" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="modules.html" class="btn btn-neutral float-right" title="Docs" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Philippe Goulet Coulombe and Ryan Lucas.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>